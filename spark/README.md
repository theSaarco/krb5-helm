# Spark in k8s mode supporting secure HDFS storage

It seems that running Spark stand-alone against secure HDFS does not work. The executors have no facility to authenticate with Kerberos, and hence cannot access files on a secure HDFS cluster. However, it is possible to execute the same with Spark in k8s mode. This was tested against Spark 3.0, as 3.0 has some additional features that are not present in 2.4.X, and are necessary for this functionality to work.
To make this work, you need to execute the following steps.

## Pre-requisite: Deploy Jupyter with Spark support

Through Iguazio UI, enable a Spark service, and then create a Jupyter service that uses the same. Once this is done, you have the Jupyter service with Spark pre-deployed in it.
The pod running the Spark driver (which is the Jupyter pod) will need to have permissions to perform (see <https://spark.apache.org/docs/latest/running-on-kubernetes.html#prerequisites>) list, create, edit and delete pods in the cluster. OOTB, the Jupyter service has all of them except for edit. This means that you need to:

1. Add pod edit permissions to the `jupyter-basic-job-executor` role
    > Note: while the documentation calls for this, it looks like everything works even without it. However, it's recommended that we add it, just to make sure.

## Create Spark Docker images

The k8s mode for Spark deployed the executors in pods that Spark generates based on an image that the caller provides.
Spark does provide a facility to generate the correct Docker images, which include the various packages needed. It should be noted that to run Python code, a different image is needed (which also can be generated by the same script).
Jupyter does not have Docker installed on it, so for now I performed the following on the k8s node itself:

1. Retrieve the Spark deployment from the Jupyter pod - basically `tar` everything under `/spark` and copy it to the node.
   > It's critical that it's the same Spark distribution and even minor version. I started with downloading Spark 3.0.1 while Jupyter runs 3.0.0, and the executors could not communicate with the driver.

2. Under the directory where you downloaded the Spark distribution, perform the following command:

   ```bash
   ./bin/docker-image-tool.sh -r spark-exec -t latest -u 1000 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build
   ```
  
   This generates 2 images: `spark-exec/spark` and `spark-exec/spark-py`. The `-r` and `-t` parameters tell the command how to label the images. The `-p` parameter tells it to create the Python image as well as the basic image. The `-u` command tells it to set `USER` for the docker image to be 1000, which is the same user other containers use for `iguazio` - this turned out to be critical, otherwise the `v3iod` communication failed due to files in `shm` not created due to wrong permissions.

The command has other parameters which for example allow you to perform various modifications to the image, and also to push the images to a repository - in our case it's not necessary since we're using the local Docker repository.

## Prepare Jupyter to execute code and copy files

For Spark to work properly against a secure HDFS we need the following files available on Jupyter and on the executors:

1. Kerberos configurations - `krb5.conf`, `krb5.keytab`
2. HDFS configurations - `core-site.xml` and `hdfs-site.xml`
3. Executor pod customization template - the executors will need access to `/User` which means that they will need fuse mounted to them. This is no different than any other pod that needs file-system access via fuse. To enable this we need to mount volumes to the pods, and while Spark has some support for that, it does not support `FlexVolume` mounts. Therefore, we need to use a template that adds these configurations to executor pods. For that we have this [yaml file](./worker_pod.yaml) which needs to be accessible to the Jupyter driver

Besides those files, there are also a [notebook file](./spark-k8s.ipynb) that has the Spark code needed to perform HDFS access, and a nice [script](create_jupyter_env) that creates a `/User/spark` directory on fuse, and populates it with the needed files. Some of these files are retrieved from the `hadoop-worker` pod, so make sure you have it running.
Once you have configured everything, you can go ahead and run the notebook from Jupyter, and as always - pray. Pray a lot.

> ## BUG: Having to place `krb5.conf` in `/etc`
>
> One huge caveat we have so far is that on the Jupyter pod, the driver for some reason requires that the `krb5.conf` file be present in the default location on `/etc`. I found no way to work around that for now. This is still WIP.

## Actual Spark configurations

The [notebook](./spark-k8s.ipynb) contains several sections with critical configurations, without which Spark will not be able to work properly. This section explains the specific parts of it and their role in the overall execution.

### Local environment variables

The Spark driver is running on the Jupyter pod in our case, since this is client mode, which is the only mode supported for Python in Spark. The driver has responsiblity for communicating with the HDFS cluster and obtaining delegation tokens to be passed to the executors. This means that the driver needs to be able to authenticate to HDFS using Kerberos. To do that, it needs the "usual suspects" of environment variables:

1. `KRB5CCNAME` - the location of the Kerberos ticket cache file
2. `HADOOP_CONF_DIR` - HDFS configuration. Without this it won't understand it needs to work securely, and who to talk to
3. `KRB5_CONFIG` - location of the Kerberos configuration. Note that per the BUG mentioned above, this has no effect for some reason

The notebook also contains code to run `kinit` to make sure it's authenticated with Kerberos. In the final product the plan is for this to be replaced with a sidecar implementation or something else.

### Spark session configurations

This section will explain what's in this big clause:

```bash
spark = SparkSession.builder.appName("Example") \
    .master('k8s://https://kubernetes.default.svc:443') \
    .config('fs.v3io.impl','io.iguaz.v3io.hcfs.V3IOFileSystem') \
    .config('fs.AbstractFileSystem.v3io.impl','io.iguaz.v3io.hcfs.V3IOAbstractFileSystem') \
    .config('spark.kubernetes.container.image','spark-exec/spark-py:latest') \
    .config('spark.kubernetes.driver.pod.name', hostname) \
    .config('spark.kubernetes.namespace','default-tenant') \
    .config('spark.pyspark.python','python3.7') \
    .config('spark.kubernetes.executor.podTemplateFile','/User/spark/worker_pod.yaml') \
    .config('spark.executor.extraJavaOptions', jvm_config_option) \
    .config('spark.executorEnv.HADOOP_CONF_DIR', hadoop_conf_dir) \
    .config('spark.kerberos.keytab', krb5_keytab_file) \
    .config('spark.kerberos.principal','hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM') \
    .config('spark.kubernetes.kerberos.krb5.path', krb5_config_file) \
    .config('spark.kerberos.access.hadoopFileSystems', hdfs_fs) \
    .getOrCreate()
```

It contains the following configuation parameters:

1. `.master('k8s://https://kubernetes.default.svc:443')` - this tells Spark that we work in k8s mode, and what is the address of the `k8s` API service. Note that both the `k8s://` and `https://` clauses are needed - if only the `k8s` part is kept, Spark will attempt `http` and fail in our case
2. `'fs.v3io.impl'`, `'fs.AbstractFileSystem.v3io.impl'` - v3io is actually overriding HDFS configurations to enable accessing files through the `v3io://` prefix. When using HDFS, some stepping-on-toes happens and Spark doesn't have these configurations available through config files. Therefore, they need to be provided through configurations
3. `'spark.kubernetes.container.image'`, `'spark.kubernetes.namespace'` - configurations that tell Spark what Docker image to use, and what namespace to use. The default namespace is `default`, so that won't work in our case. Of course, one can automatically take the value from `/var/run/secrets/kubernetes.io/serviceaccount/namespace` or some env. variable. Currently I've used a manually-provided value
4. `'spark.pyspark.python'` - the Python image has both Python 2 and Python 3 implementations on it (possibly this can be overriden in the image creation script). By default it tries to run the code using Python 2, so this parameter is needed to make it use the right one.
5. `'spark.kubernetes.executor.podTemplateFile'` - this is the Pod template file, described above
6. `'spark.kubernetes.driver.pod.name'` - this parameter is not mandatory, but it makes our pod the owner of the executor pods, so that if the Jupyter pod is deleted all executor pods will also get deleted by k8s
7. `'spark.executor.extraJavaOptions'` - I use this to pass the `java.security.krb5.conf` parameter to the executor JVM. This is also passed through the `'spark.kubernetes.kerberos.krb5.path'` parameter, so one of them seems to be redundant. Will need to check which one...
8. `'spark.executorEnv.HADOOP_CONF_DIR'` - tells the executor where to find the Hadoop configuration. Critical parameter
9. `'spark.kerberos.keytab'` and `'spark.kerberos.principal'` - these of course are critical so that Spark knows how to authenticate to Kerberos, and enable renewing of the Kerberos tickets once they expire (for long running tasks)
10. `'spark.kerberos.access.hadoopFileSystems'` - still need to verify whether this is really critical

The notebook also has a small section on some other confgurations which exist and may or not bring any benefit at all.